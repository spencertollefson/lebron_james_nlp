{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# timeframe: 2011-02-21 to 2018-09-09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "\n",
    "import joblib\n",
    "\n",
    "pd.options.display.colheader_justify = 'left'\n",
    "pd.options.display.column_space = 5\n",
    "pd.options.display.expand_frame_repr\n",
    "pd.options.display.max_colwidth = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate different data files into single CSV and import into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_master = False\n",
    "while build_master:\n",
    "    path = 'data/reddit_api_pulled_csvs/'\n",
    "    allfiles = glob.glob(path + '*.csv')\n",
    "    df = pd.DataFrame()\n",
    "    list_ = []\n",
    "    for file_ in allfiles:\n",
    "        temp = pd.read_csv(file_,index_col=None, header=0)\n",
    "        list_.append(temp)\n",
    "    df = pd.concat(list_)\n",
    "\n",
    "    df.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "    df.to_csv('data/raw/ten_comments_raw.csv')\n",
    "    break\n",
    "# simply import pre-concatenated CSV into a DF\n",
    "while not build_master:\n",
    "    path = 'data/raw/ten_comments_raw.csv'\n",
    "    df = pd.read_csv(path) #, dtype = {'created_utc': np.float64})\n",
    "    df.drop(columns='Unnamed: 0', inplace=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['score', 'body'], inplace=True)\n",
    "df.drop(index = df[df.score.str.contains('[a-zA-Z]', regex=True)].index, inplace=True) # drop any string characters that ended up in score\n",
    "\n",
    "# Convert dtypes\n",
    "df.score = df.score.astype('int')\n",
    "df.post_score = df.post_score.astype('int')\n",
    "df.created_utc = df.created_utc.astype('float')\n",
    "df['created_utc'] = df['created_utc'].apply(np.int64)\n",
    "df['created_loc'] = df['created_loc'].apply(np.int64)\n",
    "\n",
    "# Convert unix to datetime\n",
    "df.created_loc = pd.to_datetime(df.created_loc,unit='s')\n",
    "df.created_utc = pd.to_datetime(df.created_utc,unit='s')\n",
    "\n",
    "# Drop removed and deleted reddit comments\n",
    "df.body = df.body.drop(index = df[df.body == '[deleted]'].index)\n",
    "df.body = df.body.drop(index = df[df.body == '[removed]'].index)\n",
    "\n",
    "# Drop duplicate comments\n",
    "df.drop_duplicates(subset='comment_id', inplace=True)\n",
    "\n",
    "utils.string_clean_df_column(df, 'body')\n",
    "\n",
    "#Drop NAs\n",
    "df.dropna(subset=['body'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/clean/clean_df.joblib']"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(df, 'data/clean/clean_df.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
